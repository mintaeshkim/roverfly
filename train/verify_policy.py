import os, sys
current_dir = os.path.dirname(__file__)
parent_dir = os.path.abspath(os.path.join(current_dir, '..'))
sys.path.append(os.path.join(parent_dir))
sys.path.append(os.path.join(parent_dir, 'envs'))
from envs.quadrotor_env import QuadrotorEnv

import numpy as np
import torch as th
from stable_baselines3 import PPO


activation_fn = th.nn.Tanh
net_arch = {'pi': [128,128],
            'vf': [128,128]}
env = QuadrotorEnv()

model = PPO('MlpPolicy',
            env=env,
            policy_kwargs={'activation_fn':activation_fn, 'net_arch':net_arch})
state_dict = th.load("saved_models/saved_model_quadrotor_hover_100hz_1/best_model/policy.pth", map_location=th.device('cpu'))
model.policy.load_state_dict(state_dict)

obs = np.array([[
    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 
    1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 
    0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
]]).astype(np.float32)
action, states = model.predict(obs)

print(action)
print(states)